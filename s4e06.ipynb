{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 1"
      ],
      "metadata": {
        "id": "SFz-9ZwILbl0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoRiqXz05JKd"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "import numpy as np\n",
        "import os\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import missingno as msno\n",
        "from prettytable import PrettyTable\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.set(style='darkgrid', font_scale=1.4)\n",
        "from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm as tqdm_notebook\n",
        "tqdm_notebook.get_lock().locks = []\n",
        "# !pip install sweetviz\n",
        "# import sweetviz as sv\n",
        "import concurrent.futures\n",
        "from copy import deepcopy\n",
        "from functools import partial\n",
        "from itertools import combinations\n",
        "import random\n",
        "from random import randint, uniform\n",
        "import gc\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler,PowerTransformer, FunctionTransformer\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from itertools import combinations\n",
        "from sklearn.impute import SimpleImputer\n",
        "import xgboost as xg\n",
        "from sklearn.model_selection import train_test_split,cross_val_score\n",
        "from sklearn.metrics import mean_squared_error,mean_squared_log_error, roc_auc_score, accuracy_score, f1_score, precision_recall_curve, log_loss\n",
        "from sklearn.cluster import KMeans\n",
        "!pip install yellowbrick\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "!pip install gap-stat\n",
        "from gap_statistic.optimalK import OptimalK\n",
        "from scipy import stats\n",
        "import statsmodels.api as sm\n",
        "from scipy.stats import ttest_ind\n",
        "from scipy.stats import boxcox\n",
        "import math\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "!pip install optuna\n",
        "import optuna\n",
        "!pip install cmaes\n",
        "import cmaes\n",
        "import xgboost as xgb\n",
        "!pip install catboost\n",
        "!pip install lightgbm --install-option=--gpu --install-option=\"--boost-root=C:/local/boost_1_69_0\" --install-option=\"--boost-librarydir=C:/local/boost_1_69_0/lib64-msvc-14.1\"\n",
        "import lightgbm as lgb\n",
        "!pip install category_encoders\n",
        "from category_encoders import OneHotEncoder, OrdinalEncoder, CountEncoder, CatBoostEncoder\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, GradientBoostingClassifier,ExtraTreesClassifier, AdaBoostClassifier, HistGradientBoostingRegressor\n",
        "!pip install -U imbalanced-learn\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from catboost import CatBoost, CatBoostRegressor, CatBoostClassifier\n",
        "from sklearn.svm import NuSVC, SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifierCV, LogisticRegressionCV\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from catboost import Pool\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.pandas.set_option('display.max_columns',None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "table = PrettyTable()\n",
        "\n",
        "table.field_names = ['Feature', 'Data Type', 'Train Missing %', 'Test Missing %',\"Original Missing%\"]\n",
        "for column in train_copy.columns:\n",
        "    data_type = str(train_copy[column].dtype)\n",
        "    non_null_count_train= np.round(100-train_copy[column].count()/train_copy.shape[0]*100,1)\n",
        "    if column!=target:\n",
        "        non_null_count_test = np.round(100-test_copy[column].count()/test_copy.shape[0]*100,1)\n",
        "    else:\n",
        "        non_null_count_test=\"NA\"\n",
        "    non_null_count_orig= np.round(100-original_copy[column].count()/original_copy.shape[0]*100,1)\n",
        "    table.add_row([column, data_type, non_null_count_train,non_null_count_test,non_null_count_orig])\n",
        "print(table)"
      ],
      "metadata": {
        "id": "8aIQMMcDLmen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_pie_chart(data, title, ax):\n",
        "    data_counts = data[target].value_counts()\n",
        "    labels = data_counts.index\n",
        "    sizes = data_counts.values\n",
        "    colors = [ (0.2, 0.9, 0.6), 'crimson', (0.8, 0.5, 0.3)]\n",
        "    explode = (0.1,0.05, 0)\n",
        "\n",
        "    ax.pie(sizes,explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140)\n",
        "    ax.axis('equal')\n",
        "    ax.set_title(title)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 6))  # Create three subplots in a row\n",
        "\n",
        "plot_pie_chart(train_copy, \"Train Churn Distribution\", axes[0])\n",
        "plot_pie_chart(original, \"Original Churn Distribution\", axes[1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UYwEYXy-dLRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_cols = [f for f in train.columns if (train[f].dtype != 'O' and train[f].nunique() <100) or (train[f].dtype == 'O' and f not in [target]) ]\n",
        "custom_palette = (0.2, 0.9, 0.6), 'crimson', (0.8, 0.5, 0.3)\n",
        "for col in cat_cols:\n",
        "    contingency_table = pd.crosstab(train[col], train[target], normalize='index')\n",
        "    sns.set(style=\"whitegrid\")\n",
        "    contingency_table.plot(kind=\"bar\", stacked=True, color=custom_palette,figsize=(20, 4))\n",
        "    plt.title(f\"Percentage Distribution of Target across {col}\")\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel(\"Percentage\")\n",
        "    plt.legend(title=\"Target Class\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "A6c0cg79dSDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_map={\n",
        "'Graduate':0,\n",
        "'Enrolled':1,\n",
        "'Dropout':2}\n",
        "lgb_params = {\n",
        "    'n_estimators': 50,\n",
        "    'max_depth': 8,\n",
        "    'learning_rate': 0.02,\n",
        "    'subsample': 0.20,\n",
        "    'colsample_bytree': 0.56,\n",
        "    'reg_alpha': 0.25,\n",
        "    'reg_lambda': 5e-08,\n",
        "    'objective': 'multiclass',\n",
        "    'metric': 'multi_logloss',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'random_state': 42,\n",
        "    'device':device.lower(),\n",
        "    'verbose':-1\n",
        "    }\n",
        "\n",
        "def encode(y,target_map):\n",
        "    '''\n",
        "    To convert the outputs to numbers\n",
        "    '''\n",
        "    y=np.array(y)\n",
        "    encoded_y=[target_map[f] for f in y]\n",
        "    return encoded_y\n",
        "def decode(y,target_map):\n",
        "    '''To convert the predictions back to classes\n",
        "    '''\n",
        "    y=np.array(y)\n",
        "    reverse_dict={v: k for k, v in target_map.items()}\n",
        "    decoded_y=[reverse_dict[f] for f in y]\n",
        "    return decoded_y\n",
        "def min_max_scaler(train, test, column):\n",
        "    '''\n",
        "    Min Max just based on train might have an issue if test has extreme values, hence changing the denominator using overall min and max\n",
        "    '''\n",
        "    max_val=max(train[column].max(),test[column].max())\n",
        "    min_val=min(train[column].min(),test[column].min())\n",
        "\n",
        "    train[column]=(train[column]-min_val)/(max_val-min_val)\n",
        "    test[column]=(test[column]-min_val)/(max_val-min_val)\n",
        "\n",
        "    return train,test\n",
        "\n",
        "hgbm_params={'learning_rate': 0.07590136944725749, 'max_depth': 6, 'min_samples_leaf': 12, 'max_leaf_nodes': 5}\n",
        "lgb_params1 = {\n",
        "            'n_estimators': 50,\n",
        "            'max_depth': 6,\n",
        "            \"num_leaves\": 16,\n",
        "            'learning_rate': 0.002,\n",
        "            'subsample': 0.7,\n",
        "            'colsample_bytree': 0.8,\n",
        "            #'reg_alpha': 0.25,\n",
        "#             'reg_lambda': 5e-07,\n",
        "            \"boosting_type\": \"gbdt\",\n",
        "            \"objective\":'regression_l1',\n",
        "            'metric': 'mean_absolute_error',\n",
        "            'device': device.lower(),\n",
        "            'random_state': 42,\n",
        "            'verbose':-1\n",
        "        }\n",
        "def store_missing_rows(df, features):\n",
        "    '''Function stores where missing values are located for given set of features'''\n",
        "    missing_rows = {}\n",
        "\n",
        "    for feature in features:\n",
        "        missing_rows[feature] = df[df[feature].isnull()]\n",
        "\n",
        "    return missing_rows\n",
        "def fill_missing_numerical(train,test,target, max_iterations=10):\n",
        "    '''Iterative Missing Imputer: Updates filled missing values iteratively using CatBoost Algorithm'''\n",
        "    train_temp=train.copy()\n",
        "    if target in train_temp.columns:\n",
        "        train_temp=train_temp.drop(columns=target)\n",
        "\n",
        "\n",
        "    df=pd.concat([train_temp,test],axis=\"rows\")\n",
        "    df=df.reset_index(drop=True)\n",
        "    features=[ f for f in df.columns if df[f].isna().sum()>0]\n",
        "    if len(features)>0:\n",
        "        # Step 1: Store the instances with missing values in each feature\n",
        "        missing_rows = store_missing_rows(df, features)\n",
        "\n",
        "        # Step 2: Initially fill all missing values with \"Missing\"\n",
        "        for f in features:\n",
        "            df[f]=df[f].fillna(df[f].median())\n",
        "\n",
        "        cat_features=[f for f in df.columns if not pd.api.types.is_numeric_dtype(df[f])]\n",
        "        dictionary = {feature: [] for feature in features}\n",
        "\n",
        "        for iteration in tqdm(range(max_iterations), desc=\"Iterations\"):\n",
        "            for feature in features:\n",
        "#                 print(feature)\n",
        "                # Skip features with no missing values\n",
        "                rows_miss = missing_rows[feature].index\n",
        "                replace_dict={}\n",
        "                rev_replace_dict={}\n",
        "                for col in  cat_features:\n",
        "                    df[col]=df[col].astype(str)\n",
        "                    int_cat=dict(zip(df[col].unique(),np.arange(0, df[col].nunique())))\n",
        "                    rev_int_cat=dict(zip(np.arange(0, df[col].nunique()), df[col].unique()))\n",
        "                    df[col]=df[col].replace(int_cat)\n",
        "\n",
        "                    replace_dict[col]=int_cat\n",
        "                    rev_replace_dict[col]=rev_int_cat\n",
        "\n",
        "                missing_temp = df.loc[rows_miss].copy()\n",
        "                non_missing_temp = df.drop(index=rows_miss).copy()\n",
        "                y_pred_prev=missing_temp[feature]\n",
        "                missing_temp = missing_temp.drop(columns=[feature])\n",
        "\n",
        "\n",
        "                # Step 3: Use the remaining features to predict missing values using Random Forests\n",
        "                X_train = non_missing_temp.drop(columns=[feature])\n",
        "                y_train = non_missing_temp[[feature]]\n",
        "\n",
        "                model1 =HistGradientBoostingRegressor(**hgbm_params, max_iter=100, loss=\"absolute_error\", n_iter_no_change=50,random_state=42)\n",
        "                model1.fit(X_train, y_train)\n",
        "\n",
        "                model2 = lgb.LGBMRegressor(**lgb_params1)\n",
        "                model2.fit(X_train, y_train)\n",
        "\n",
        "                # Step 4: Predict missing values for the feature and update all N features\n",
        "                y_pred = (np.array(model1.predict(missing_temp))+np.array(model2.predict(missing_temp)))/2\n",
        "                df.loc[rows_miss, feature] = y_pred\n",
        "#                 error_minimize=rmse(y_pred,y_pred_prev) #mean_squared_error\n",
        "                error_minimize=mean_squared_error(y_pred,y_pred_prev) #mean_squared_error\n",
        "                dictionary[feature].append(error_minimize)  # Append the error_minimize value\n",
        "\n",
        "                for col in  cat_features:\n",
        "                    df[col]=df[col].replace(rev_int_cat)\n",
        "\n",
        "\n",
        "        for feature, values in dictionary.items():\n",
        "            values=np.array(values)/sum(values)\n",
        "            iterations = range(1, len(values) + 1)  # x-axis values (iterations)\n",
        "            plt.plot(iterations, values, label=feature)  # plot the values\n",
        "            plt.xlabel('Iterations')\n",
        "            plt.ylabel('RMSE')\n",
        "            plt.title('Minimization of RMSE with iterations')\n",
        "            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        plt.show()\n",
        "        train[features] = np.array(df.iloc[:train.shape[0]][features])\n",
        "        test[features] = np.array(df.iloc[train.shape[0]:][features])\n",
        "\n",
        "    return train,test"
      ],
      "metadata": {
        "id": "L3r8G2U0oGjR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}