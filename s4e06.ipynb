{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "SFz-9ZwILbl0"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 1"
      ],
      "metadata": {
        "id": "SFz-9ZwILbl0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoRiqXz05JKd"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "import numpy as np\n",
        "import os\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import missingno as msno\n",
        "from prettytable import PrettyTable\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.set(style='darkgrid', font_scale=1.4)\n",
        "from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm as tqdm_notebook\n",
        "tqdm_notebook.get_lock().locks = []\n",
        "# !pip install sweetviz\n",
        "# import sweetviz as sv\n",
        "import concurrent.futures\n",
        "from copy import deepcopy\n",
        "from functools import partial\n",
        "from itertools import combinations\n",
        "import random\n",
        "from random import randint, uniform\n",
        "import gc\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler,PowerTransformer, FunctionTransformer\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from itertools import combinations\n",
        "from sklearn.impute import SimpleImputer\n",
        "import xgboost as xg\n",
        "from sklearn.model_selection import train_test_split,cross_val_score\n",
        "from sklearn.metrics import mean_squared_error,mean_squared_log_error, roc_auc_score, accuracy_score, f1_score, precision_recall_curve, log_loss\n",
        "from sklearn.cluster import KMeans\n",
        "!pip install yellowbrick\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "!pip install gap-stat\n",
        "from gap_statistic.optimalK import OptimalK\n",
        "from scipy import stats\n",
        "import statsmodels.api as sm\n",
        "from scipy.stats import ttest_ind\n",
        "from scipy.stats import boxcox\n",
        "import math\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "!pip install optuna\n",
        "import optuna\n",
        "!pip install cmaes\n",
        "import cmaes\n",
        "import xgboost as xgb\n",
        "!pip install catboost\n",
        "!pip install lightgbm --install-option=--gpu --install-option=\"--boost-root=C:/local/boost_1_69_0\" --install-option=\"--boost-librarydir=C:/local/boost_1_69_0/lib64-msvc-14.1\"\n",
        "import lightgbm as lgb\n",
        "!pip install category_encoders\n",
        "from category_encoders import OneHotEncoder, OrdinalEncoder, CountEncoder, CatBoostEncoder\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, GradientBoostingClassifier,ExtraTreesClassifier, AdaBoostClassifier, HistGradientBoostingRegressor\n",
        "!pip install -U imbalanced-learn\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from catboost import CatBoost, CatBoostRegressor, CatBoostClassifier\n",
        "from sklearn.svm import NuSVC, SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifierCV, LogisticRegressionCV\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from catboost import Pool\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.pandas.set_option('display.max_columns',None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "table = PrettyTable()\n",
        "\n",
        "table.field_names = ['Feature', 'Data Type', 'Train Missing %', 'Test Missing %',\"Original Missing%\"]\n",
        "for column in train_copy.columns:\n",
        "    data_type = str(train_copy[column].dtype)\n",
        "    non_null_count_train= np.round(100-train_copy[column].count()/train_copy.shape[0]*100,1)\n",
        "    if column!=target:\n",
        "        non_null_count_test = np.round(100-test_copy[column].count()/test_copy.shape[0]*100,1)\n",
        "    else:\n",
        "        non_null_count_test=\"NA\"\n",
        "    non_null_count_orig= np.round(100-original_copy[column].count()/original_copy.shape[0]*100,1)\n",
        "    table.add_row([column, data_type, non_null_count_train,non_null_count_test,non_null_count_orig])\n",
        "print(table)"
      ],
      "metadata": {
        "id": "8aIQMMcDLmen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_pie_chart(data, title, ax):\n",
        "    data_counts = data[target].value_counts()\n",
        "    labels = data_counts.index\n",
        "    sizes = data_counts.values\n",
        "    colors = [ (0.2, 0.9, 0.6), 'crimson', (0.8, 0.5, 0.3)]\n",
        "    explode = (0.1,0.05, 0)\n",
        "\n",
        "    ax.pie(sizes,explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140)\n",
        "    ax.axis('equal')\n",
        "    ax.set_title(title)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 6))  # Create three subplots in a row\n",
        "\n",
        "plot_pie_chart(train_copy, \"Train Churn Distribution\", axes[0])\n",
        "plot_pie_chart(original, \"Original Churn Distribution\", axes[1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UYwEYXy-dLRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_cols = [f for f in train.columns if (train[f].dtype != 'O' and train[f].nunique() <100) or (train[f].dtype == 'O' and f not in [target]) ]\n",
        "custom_palette = (0.2, 0.9, 0.6), 'crimson', (0.8, 0.5, 0.3)\n",
        "for col in cat_cols:\n",
        "    contingency_table = pd.crosstab(train[col], train[target], normalize='index')\n",
        "    sns.set(style=\"whitegrid\")\n",
        "    contingency_table.plot(kind=\"bar\", stacked=True, color=custom_palette,figsize=(20, 4))\n",
        "    plt.title(f\"Percentage Distribution of Target across {col}\")\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel(\"Percentage\")\n",
        "    plt.legend(title=\"Target Class\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "A6c0cg79dSDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_map={\n",
        "'Graduate':0,\n",
        "'Enrolled':1,\n",
        "'Dropout':2}\n",
        "lgb_params = {\n",
        "    'n_estimators': 50,\n",
        "    'max_depth': 8,\n",
        "    'learning_rate': 0.02,\n",
        "    'subsample': 0.20,\n",
        "    'colsample_bytree': 0.56,\n",
        "    'reg_alpha': 0.25,\n",
        "    'reg_lambda': 5e-08,\n",
        "    'objective': 'multiclass',\n",
        "    'metric': 'multi_logloss',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'random_state': 42,\n",
        "    'device':device.lower(),\n",
        "    'verbose':-1\n",
        "    }\n",
        "\n",
        "def encode(y,target_map):\n",
        "    '''\n",
        "    To convert the outputs to numbers\n",
        "    '''\n",
        "    y=np.array(y)\n",
        "    encoded_y=[target_map[f] for f in y]\n",
        "    return encoded_y\n",
        "def decode(y,target_map):\n",
        "    '''To convert the predictions back to classes\n",
        "    '''\n",
        "    y=np.array(y)\n",
        "    reverse_dict={v: k for k, v in target_map.items()}\n",
        "    decoded_y=[reverse_dict[f] for f in y]\n",
        "    return decoded_y\n",
        "def min_max_scaler(train, test, column):\n",
        "    '''\n",
        "    Min Max just based on train might have an issue if test has extreme values, hence changing the denominator using overall min and max\n",
        "    '''\n",
        "    max_val=max(train[column].max(),test[column].max())\n",
        "    min_val=min(train[column].min(),test[column].min())\n",
        "\n",
        "    train[column]=(train[column]-min_val)/(max_val-min_val)\n",
        "    test[column]=(test[column]-min_val)/(max_val-min_val)\n",
        "\n",
        "    return train,test\n",
        "\n",
        "hgbm_params={'learning_rate': 0.07590136944725749, 'max_depth': 6, 'min_samples_leaf': 12, 'max_leaf_nodes': 5}\n",
        "lgb_params1 = {\n",
        "            'n_estimators': 50,\n",
        "            'max_depth': 6,\n",
        "            \"num_leaves\": 16,\n",
        "            'learning_rate': 0.002,\n",
        "            'subsample': 0.7,\n",
        "            'colsample_bytree': 0.8,\n",
        "            #'reg_alpha': 0.25,\n",
        "#             'reg_lambda': 5e-07,\n",
        "            \"boosting_type\": \"gbdt\",\n",
        "            \"objective\":'regression_l1',\n",
        "            'metric': 'mean_absolute_error',\n",
        "            'device': device.lower(),\n",
        "            'random_state': 42,\n",
        "            'verbose':-1\n",
        "        }\n",
        "def store_missing_rows(df, features):\n",
        "    '''Function stores where missing values are located for given set of features'''\n",
        "    missing_rows = {}\n",
        "\n",
        "    for feature in features:\n",
        "        missing_rows[feature] = df[df[feature].isnull()]\n",
        "\n",
        "    return missing_rows\n",
        "def fill_missing_numerical(train,test,target, max_iterations=10):\n",
        "    '''Iterative Missing Imputer: Updates filled missing values iteratively using CatBoost Algorithm'''\n",
        "    train_temp=train.copy()\n",
        "    if target in train_temp.columns:\n",
        "        train_temp=train_temp.drop(columns=target)\n",
        "\n",
        "\n",
        "    df=pd.concat([train_temp,test],axis=\"rows\")\n",
        "    df=df.reset_index(drop=True)\n",
        "    features=[ f for f in df.columns if df[f].isna().sum()>0]\n",
        "    if len(features)>0:\n",
        "        # Step 1: Store the instances with missing values in each feature\n",
        "        missing_rows = store_missing_rows(df, features)\n",
        "\n",
        "        # Step 2: Initially fill all missing values with \"Missing\"\n",
        "        for f in features:\n",
        "            df[f]=df[f].fillna(df[f].median())\n",
        "\n",
        "        cat_features=[f for f in df.columns if not pd.api.types.is_numeric_dtype(df[f])]\n",
        "        dictionary = {feature: [] for feature in features}\n",
        "\n",
        "        for iteration in tqdm(range(max_iterations), desc=\"Iterations\"):\n",
        "            for feature in features:\n",
        "#                 print(feature)\n",
        "                # Skip features with no missing values\n",
        "                rows_miss = missing_rows[feature].index\n",
        "                replace_dict={}\n",
        "                rev_replace_dict={}\n",
        "                for col in  cat_features:\n",
        "                    df[col]=df[col].astype(str)\n",
        "                    int_cat=dict(zip(df[col].unique(),np.arange(0, df[col].nunique())))\n",
        "                    rev_int_cat=dict(zip(np.arange(0, df[col].nunique()), df[col].unique()))\n",
        "                    df[col]=df[col].replace(int_cat)\n",
        "\n",
        "                    replace_dict[col]=int_cat\n",
        "                    rev_replace_dict[col]=rev_int_cat\n",
        "\n",
        "                missing_temp = df.loc[rows_miss].copy()\n",
        "                non_missing_temp = df.drop(index=rows_miss).copy()\n",
        "                y_pred_prev=missing_temp[feature]\n",
        "                missing_temp = missing_temp.drop(columns=[feature])\n",
        "\n",
        "\n",
        "                # Step 3: Use the remaining features to predict missing values using Random Forests\n",
        "                X_train = non_missing_temp.drop(columns=[feature])\n",
        "                y_train = non_missing_temp[[feature]]\n",
        "\n",
        "                model1 =HistGradientBoostingRegressor(**hgbm_params, max_iter=100, loss=\"absolute_error\", n_iter_no_change=50,random_state=42)\n",
        "                model1.fit(X_train, y_train)\n",
        "\n",
        "                model2 = lgb.LGBMRegressor(**lgb_params1)\n",
        "                model2.fit(X_train, y_train)\n",
        "\n",
        "                # Step 4: Predict missing values for the feature and update all N features\n",
        "                y_pred = (np.array(model1.predict(missing_temp))+np.array(model2.predict(missing_temp)))/2\n",
        "                df.loc[rows_miss, feature] = y_pred\n",
        "#                 error_minimize=rmse(y_pred,y_pred_prev) #mean_squared_error\n",
        "                error_minimize=mean_squared_error(y_pred,y_pred_prev) #mean_squared_error\n",
        "                dictionary[feature].append(error_minimize)  # Append the error_minimize value\n",
        "\n",
        "                for col in  cat_features:\n",
        "                    df[col]=df[col].replace(rev_int_cat)\n",
        "\n",
        "\n",
        "        for feature, values in dictionary.items():\n",
        "            values=np.array(values)/sum(values)\n",
        "            iterations = range(1, len(values) + 1)  # x-axis values (iterations)\n",
        "            plt.plot(iterations, values, label=feature)  # plot the values\n",
        "            plt.xlabel('Iterations')\n",
        "            plt.ylabel('RMSE')\n",
        "            plt.title('Minimization of RMSE with iterations')\n",
        "            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        plt.show()\n",
        "        train[features] = np.array(df.iloc[:train.shape[0]][features])\n",
        "        test[features] = np.array(df.iloc[train.shape[0]:][features])\n",
        "\n",
        "    return train,test"
      ],
      "metadata": {
        "id": "L3r8G2U0oGjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_cols = [f for f in test.columns if (train[f].dtype != 'O' and train[f].nunique()<100 and train[f].nunique()>2) or (train[f].dtype == 'O') ]\n",
        "print(train[cat_cols].nunique())"
      ],
      "metadata": {
        "id": "8jrdwZLyoIgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nearest_val(target):\n",
        "    return min(common, key=lambda x: abs(x - target))\n",
        "\n",
        "cat_cols_updated=[]\n",
        "for col in cat_cols:\n",
        "    uncommon=list((set(test[col].unique())| set(train[col].unique()))-(set(test[col].unique())& set(train[col].unique())))\n",
        "\n",
        "    if train[col].dtype!=\"O\":\n",
        "        train[f\"{col}_cat\"]=train[col]\n",
        "        test[f\"{col}_cat\"]=test[col]\n",
        "        cat_cols_updated.append(f\"{col}_cat\")\n",
        "        if uncommon:\n",
        "            common=list(set(test[col].unique())& set(train[col].unique()))\n",
        "            train[f\"{col}_cat\"]=train[col].apply(lambda x: np.nan if x in uncommon else x)#train[col].apply(nearest_val)\n",
        "            test[f\"{col}_cat\"]=test[col].apply(lambda x: np.nan if x in uncommon else x)#test[col].apply(nearest_val)\n",
        "    else:\n",
        "        cat_cols_updated.append(col)\n",
        "        train[col]=train[col].astype(str)+\"_\"+col\n",
        "        test[col]=test[col].astype(str)+\"_\"+col\n",
        "        uncommon=list((set(test[col].unique())| set(train[col].unique()))-(set(test[col].unique())& set(train[col].unique())))\n",
        "        train[col]=train[col].apply(lambda x: np.nan if x in uncommon else x)\n",
        "        test[col]=test[col].apply(lambda x: np.nan if x in uncommon else x)\n",
        "\n",
        "print(train[cat_cols_updated].nunique())"
      ],
      "metadata": {
        "id": "l52wl091oJvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global overall_best_score\n",
        "overall_best_score = 0\n",
        "def OHE(train_df,test_df,cols,target):\n",
        "    '''\n",
        "    Function for one hot encoding, it first combines the data so that no category is missed and\n",
        "    the category with least frequency can be dropped because of redundancy\n",
        "    '''\n",
        "    combined = pd.concat([train_df, test_df], axis=0)\n",
        "    for col in cols:\n",
        "        one_hot = pd.get_dummies(combined[col]).astype(int)\n",
        "        counts = combined[col].value_counts()\n",
        "        min_count_category = counts.idxmin()\n",
        "        one_hot = one_hot.drop(min_count_category, axis=1)\n",
        "        one_hot.columns=[str(f)+col+\"_OHE\" for f in one_hot.columns]\n",
        "        combined = pd.concat([combined, one_hot], axis=\"columns\")\n",
        "        combined = combined.loc[:, ~combined.columns.duplicated()]\n",
        "\n",
        "    # split back to train and test dataframes\n",
        "    train_ohe = combined[:len(train_df)]\n",
        "    test_ohe = combined[len(train_df):]\n",
        "    test_ohe.reset_index(inplace=True,drop=True)\n",
        "    test_ohe.drop(columns=[target],inplace=True)\n",
        "    return train_ohe, test_ohe\n",
        "\n",
        "def high_freq_ohe(train, test, extra_cols, target, n_limit=50):\n",
        "    '''\n",
        "    If you wish to apply one hot encoding on a feature with so many unique values, then this can be applied,\n",
        "    where it takes a maximum of n categories and drops the rest of them treating as rare categories\n",
        "    '''\n",
        "    train_copy=train.copy()\n",
        "    test_copy=test.copy()\n",
        "    ohe_cols=[]\n",
        "    for col in extra_cols:\n",
        "        dict1=train_copy[col].value_counts().to_dict()\n",
        "        ordered=dict(sorted(dict1.items(), key=lambda x: x[1], reverse=True))\n",
        "        rare_keys=list([*ordered.keys()][n_limit:])\n",
        "#         ext_keys=[f[0] for f in ordered.items() if f[1]<50]\n",
        "        rare_key_map=dict(zip(rare_keys, np.full(len(rare_keys),9999)))\n",
        "\n",
        "        train_copy[col]=train_copy[col].replace(rare_key_map)\n",
        "        test_copy[col]=test_copy[col].replace(rare_key_map)\n",
        "    train_copy, test_copy = OHE(train_copy, test_copy, extra_cols, target)\n",
        "    drop_cols=[f for f in train_copy.columns if \"9999\" in f or train_copy[f].nunique()==1]\n",
        "    train_copy=train_copy.drop(columns=drop_cols)\n",
        "    test_copy=test_copy.drop(columns=drop_cols)\n",
        "\n",
        "    return train_copy, test_copy\n",
        "\n",
        "def cat_encoding(train, test,cat_cols_updated, target):\n",
        "    global overall_best_score\n",
        "    global overall_best_col\n",
        "    table = PrettyTable()\n",
        "    table.field_names = ['Feature', 'Encoded Features', 'Accuracy Score']\n",
        "    train_copy=train.copy()\n",
        "    test_copy=test.copy()\n",
        "    train_dum = train.copy()\n",
        "    for feature in cat_cols_updated:\n",
        "#         print(feature)\n",
        "#         cat_labels = train_dum.groupby([feature])[target].mean().sort_values().index\n",
        "#         cat_labels2 = {k: i for i, k in enumerate(cat_labels, 0)}\n",
        "#         train_copy[feature + \"_target\"] = train[feature].map(cat_labels2)\n",
        "#         test_copy[feature + \"_target\"] = test[feature].map(cat_labels2)\n",
        "\n",
        "        dic = train[feature].value_counts().to_dict()\n",
        "        train_copy[feature + \"_count\"] =train[feature].map(dic)\n",
        "        test_copy[feature + \"_count\"] = test[feature].map(dic)\n",
        "\n",
        "        dic2=train[feature].value_counts().to_dict()\n",
        "#         list1=np.arange(len(dic2.values()),0,-1) # Higher rank for high count\n",
        "        list1=np.arange(len(dic2.values())) # Higher rank for low count\n",
        "        dic3=dict(zip(list(dic2.keys()),list1))\n",
        "\n",
        "        train_copy[feature+\"_count_label\"]=train[feature].replace(dic3).astype(float)\n",
        "        test_copy[feature+\"_count_label\"]=test[feature].replace(dic3).astype(float)\n",
        "\n",
        "        temp_cols = [ feature + \"_count\", feature + \"_count_label\"]#,feature + \"_target\"\n",
        "\n",
        "        train_copy[feature]=train_copy[feature].astype(str)+\"_\"+feature\n",
        "        test_copy[feature]=test_copy[feature].astype(str)+\"_\"+feature\n",
        "\n",
        "        if train_copy[feature].nunique()<=10:\n",
        "            train_copy[feature]=train_copy[feature].astype(str)+\"_\"+feature\n",
        "            test_copy[feature]=test_copy[feature].astype(str)+\"_\"+feature\n",
        "            train_copy, test_copy = OHE(train_copy, test_copy, [feature], target)\n",
        "\n",
        "        else:\n",
        "            train_copy,test_copy=high_freq_ohe(train_copy,test_copy,[feature], target, n_limit=10)\n",
        "\n",
        "        train_copy=train_copy.drop(columns=[feature])\n",
        "        test_copy=test_copy.drop(columns=[feature])\n",
        "\n",
        "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "        ll_scores = []\n",
        "\n",
        "        for f in temp_cols:\n",
        "            X = train_copy[[f]].values\n",
        "            y = train_copy[target].values\n",
        "\n",
        "            log_loss_score = []\n",
        "            for train_idx, val_idx in kf.split(X, y):\n",
        "                X_train, y_train = X[train_idx], y[train_idx]\n",
        "                x_val, y_val = X[val_idx], y[val_idx]\n",
        "                model =  lgb.LGBMClassifier(**lgb_params)\n",
        "\n",
        "                model.fit(X_train, encode(y_train, target_map))\n",
        "                y_pred = model.predict(x_val)\n",
        "                log_loss_score.append(accuracy_score(encode(y_val, target_map),y_pred))\n",
        "\n",
        "            ll_scores.append((f, np.mean(log_loss_score)))\n",
        "            if overall_best_score < np.mean(log_loss_score):\n",
        "                overall_best_score = np.mean(log_loss_score)\n",
        "                overall_best_col = f\n",
        "        best_col, best_loss = sorted(ll_scores, key=lambda x: x[1], reverse=True)[0]\n",
        "\n",
        "        corr = train_copy[temp_cols].corr(method='pearson')\n",
        "        corr_with_best_col = corr[best_col]\n",
        "        cols_to_drop = [f for f in temp_cols if corr_with_best_col[f] > 0.5 and f != best_col]\n",
        "        final_selection = [f for f in temp_cols if f not in cols_to_drop]\n",
        "        if cols_to_drop:\n",
        "            train_copy = train_copy.drop(columns=cols_to_drop)\n",
        "            test_copy = test_copy.drop(columns=cols_to_drop)\n",
        "\n",
        "        table.add_row([feature, best_col, best_loss])\n",
        "\n",
        "    print(table)\n",
        "    print(\"overall best CV score: \", overall_best_score)\n",
        "    return train_copy, test_copy\n",
        "\n",
        "train, test= cat_encoding(train, test,cat_cols_updated, target)"
      ],
      "metadata": {
        "id": "iMWCgV6voPEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def better_features(train, test, target, cols, best_score):\n",
        "    new_cols = []\n",
        "    skf = KFold(n_splits=5, shuffle=True, random_state=42)  # Stratified k-fold object\n",
        "    best_list=[]\n",
        "    for i in tqdm(range(len(cols)), desc='Generating Columns'):\n",
        "        col1 = cols[i]\n",
        "        temp_df = pd.DataFrame()  # Temporary dataframe to store the generated columns\n",
        "        temp_df_test = pd.DataFrame()  # Temporary dataframe for test data\n",
        "\n",
        "        for j in range(i+1, len(cols)):\n",
        "            col2 = cols[j]\n",
        "            # Multiply\n",
        "            temp_df[col1 + '*' + col2] = train[col1] * train[col2]\n",
        "            temp_df_test[col1 + '*' + col2] = test[col1] * test[col2]\n",
        "\n",
        "            # Divide (col1 / col2)\n",
        "            temp_df[col1 + '/' + col2] = train[col1] / (train[col2] + 1e-5)\n",
        "            temp_df_test[col1 + '/' + col2] = test[col1] / (test[col2] + 1e-5)\n",
        "\n",
        "            # Divide (col2 / col1)\n",
        "            temp_df[col2 + '/' + col1] = train[col2] / (train[col1] + 1e-5)\n",
        "            temp_df_test[col2 + '/' + col1] = test[col2] / (test[col1] + 1e-5)\n",
        "\n",
        "            # Subtract\n",
        "            temp_df[col1 + '-' + col2] = train[col1] - train[col2]\n",
        "            temp_df_test[col1 + '-' + col2] = test[col1] - test[col2]\n",
        "\n",
        "            # Add\n",
        "            temp_df[col1 + '+' + col2] = train[col1] + train[col2]\n",
        "            temp_df_test[col1 + '+' + col2] = test[col1] + test[col2]\n",
        "\n",
        "        SCORES = []\n",
        "        for column in temp_df.columns:\n",
        "            scores = []\n",
        "            for train_index, val_index in skf.split(train, train[target]):\n",
        "                X_train, X_val = temp_df[column].iloc[train_index].values.reshape(-1, 1), temp_df[column].iloc[val_index].values.reshape(-1, 1)\n",
        "                y_train, y_val = train[target].iloc[train_index], train[target].iloc[val_index]\n",
        "                model =lgb.LGBMClassifier(**lgb_params)\n",
        "                model.fit(X_train, encode(y_train, target_map))\n",
        "                y_pred = model.predict(X_val)\n",
        "                score = accuracy_score(encode(y_val, target_map),y_pred)\n",
        "                scores.append(score)\n",
        "            mean_score = np.mean(scores)\n",
        "            SCORES.append((column, mean_score))\n",
        "\n",
        "        if SCORES:\n",
        "            best_col, best_acc = sorted(SCORES, key=lambda x: x[1],reverse=True)[0]\n",
        "            corr_with_other_cols = train.drop([target] + new_cols, axis=1).corrwith(temp_df[best_col])\n",
        "            if (corr_with_other_cols.abs().max() < 0.9 or best_acc > best_score) and corr_with_other_cols.abs().max() !=1 :\n",
        "                train[best_col] = temp_df[best_col]\n",
        "                test[best_col] = temp_df_test[best_col]\n",
        "                new_cols.append(best_col)\n",
        "                print(f\"Added column '{best_col}' with Accuracy Score: {best_acc:.4f} & Correlation {corr_with_other_cols.abs().max():.4f}\")\n",
        "\n",
        "    return train, test, new_cols\n"
      ],
      "metadata": {
        "id": "8D3Z3QCR-3gR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_features=[f for f in test.columns if train[f].nunique()>2]\n",
        "len(selected_features)\n",
        "\n",
        "# train, test,new_cols=better_features(train, test, target, selected_features, overall_best_score)\n",
        "# new_cols"
      ],
      "metadata": {
        "id": "_wBdbIYS-5wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_cols=['Curricular units 2nd sem (approved)/Marital status',\n",
        " 'Application mode+Curricular units 2nd sem (approved)_cat_count',\n",
        " 'Application order-Curricular units 2nd sem (approved)_cat_count',\n",
        " 'Course/Curricular units 2nd sem (approved)',\n",
        " 'Previous qualification+Curricular units 2nd sem (approved)_cat_count',\n",
        " 'Curricular units 2nd sem (approved)/Previous qualification (grade)',\n",
        " \"Mother's qualification-Curricular units 2nd sem (approved)_cat_count\",\n",
        " \"Father's qualification+Curricular units 2nd sem (approved)_cat_count\",\n",
        " \"Mother's occupation-Curricular units 2nd sem (approved)_cat_count\",\n",
        " \"Father's occupation+Curricular units 2nd sem (approved)_cat_count\",\n",
        " 'Admission grade*Curricular units 2nd sem (approved)',\n",
        " 'Age at enrollment-Curricular units 2nd sem (approved)_cat_count',\n",
        " 'Curricular units 1st sem (credited)+Curricular units 2nd sem (approved)_cat_count',\n",
        " 'Curricular units 1st sem (enrolled)/Curricular units 2nd sem (approved)',\n",
        " 'Curricular units 1st sem (evaluations)+Curricular units 2nd sem (approved)_cat_count',\n",
        " 'Curricular units 1st sem (approved)/Curricular units 2nd sem (approved)',\n",
        " 'Curricular units 1st sem (grade)*Curricular units 2nd sem (approved)',\n",
        " 'Curricular units 1st sem (without evaluations)-Curricular units 2nd sem (approved)_cat_count',\n",
        " 'Curricular units 2nd sem (credited)+Curricular units 2nd sem (approved)_cat_count',\n",
        " 'Curricular units 2nd sem (approved)/Curricular units 2nd sem (enrolled)',\n",
        " 'Curricular units 2nd sem (evaluations)-Curricular units 2nd sem (approved)_cat_count',\n",
        " 'Curricular units 2nd sem (approved)/Curricular units 1st sem (approved)_cat_count',\n",
        " 'Curricular units 2nd sem (grade)-Curricular units 2nd sem (approved)_cat_count',\n",
        " 'Curricular units 2nd sem (without evaluations)-Curricular units 2nd sem (approved)_cat_count',\n",
        " 'Unemployment rate-Curricular units 2nd sem (approved)_cat_count',\n",
        " 'Inflation rate-Curricular units 2nd sem (approved)_cat_count',\n",
        " 'GDP+Curricular units 2nd sem (approved)_cat_count',\n",
        " 'Marital status_cat_count*Curricular units 2nd sem (approved)_cat_count_label',\n",
        " 'Marital status_cat_count_label-Curricular units 2nd sem (approved)_cat_count',\n",
        " 'Application mode_cat_count*Curricular units 2nd sem (approved)_cat_count_label',\n",
        " 'Application mode_cat_count_label+Curricular units 2nd sem (approved)_cat_count',\n",
        " 'Application order_cat_count*Curricular units 2nd sem (approved)_cat_count_label',\n",
        " 'Application order_cat_count_label-Curricular units 2nd sem (approved)_cat_count',\n",
        " 'Course_cat_count+Curricular units 2nd sem (approved)_cat_count',\n",
        " 'Course_cat_count_label+Curricular units 2nd sem (approved)_cat_count',\n",
        " 'Previous qualification_cat_count*Curricular units 2nd sem (approved)_cat_count_label',\n",
        " 'Previous qualification_cat_count_label-Curricular units 2nd sem (approved)_cat_count',\n",
        " 'Nacionality_cat_count/Curricular units 2nd sem (approved)_cat_count',\n",
        " \"Mother's qualification_cat_count/Curricular units 2nd sem (approved)_cat_count_label\",\n",
        " \"Mother's qualification_cat_count_label-Curricular units 2nd sem (approved)_cat_count\",\n",
        " \"Father's qualification_cat_count*Curricular units 2nd sem (approved)_cat_count_label\",\n",
        " \"Father's qualification_cat_count_label-Curricular units 2nd sem (approved)_cat_count\",\n",
        " \"Mother's occupation_cat_count*Curricular units 2nd sem (approved)_cat_count_label\",\n",
        " \"Mother's occupation_cat_count_label-Curricular units 2nd sem (approved)_cat_count\",\n",
        " \"Father's occupation_cat_count*Curricular units 2nd sem (approved)_cat_count_label\",\n",
        " \"Father's occupation_cat_count_label+Curricular units 2nd sem (approved)_cat_count\",\n",
        " 'Curricular units 2nd sem (approved)_cat_count_label/Age at enrollment_cat_count',\n",
        " 'Age at enrollment_cat_count_label-Curricular units 2nd sem (approved)_cat_count',\n",
        " 'Curricular units 1st sem (credited)_cat_count+Curricular units 2nd sem (approved)_cat_count_label',\n",
        " 'Curricular units 1st sem (credited)_cat_count_label+Curricular units 2nd sem (approved)_cat_count',\n",
        " 'Curricular units 1st sem (enrolled)_cat_count+Curricular units 2nd sem (approved)_cat_count',\n",
        " 'Curricular units 1st sem (enrolled)_cat_count_label+Curricular units 2nd sem (approved)_cat_count',\n",
        " 'Curricular units 1st sem (evaluations)_cat_count*Curricular units 2nd sem (approved)_cat_count_label',\n",
        " 'Curricular units 1st sem (evaluations)_cat_count_label+Curricular units 2nd sem (approved)_cat_count',\n",
        " 'Curricular units 1st sem (approved)_cat_count-Curricular units 2nd sem (approved)_cat_count',\n",
        " 'Curricular units 1st sem (approved)_cat_count_label-Curricular units 2nd sem (approved)_cat_count',\n",
        " 'Curricular units 1st sem (without evaluations)_cat_count+Curricular units 2nd sem (approved)_cat_count_label',\n",
        " 'Curricular units 1st sem (without evaluations)_cat_count_label-Curricular units 2nd sem (approved)_cat_count',\n",
        " 'Curricular units 2nd sem (credited)_cat_count/Curricular units 2nd sem (approved)_cat_count_label',\n",
        " 'Curricular units 2nd sem (credited)_cat_count_label+Curricular units 2nd sem (approved)_cat_count',\n",
        " 'Curricular units 2nd sem (enrolled)_cat_count+Curricular units 2nd sem (approved)_cat_count',\n",
        " 'Curricular units 2nd sem (approved)_cat_count/Curricular units 2nd sem (enrolled)_cat_count_label',\n",
        " 'Curricular units 2nd sem (evaluations)_cat_count*Curricular units 2nd sem (approved)_cat_count_label',\n",
        " 'Curricular units 2nd sem (evaluations)_cat_count_label-Curricular units 2nd sem (approved)_cat_count',\n",
        " 'Curricular units 2nd sem (approved)_cat_count-Inflation rate_cat_count_label',\n",
        " 'Inflation rate_cat_count/Curricular units 2nd sem (without evaluations)_cat_count_label',\n",
        " 'Unemployment rate_cat_count_label*Inflation rate_cat_count']\n"
      ],
      "metadata": {
        "id": "x32rU1jP_AAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_arithmetic_operations(train_df, test_df, expressions_list):\n",
        "    for expression in expressions_list:\n",
        "        if expression not in train_df.columns:\n",
        "            # Split the expression based on operators (+, -, *, /)\n",
        "            parts = expression.split('+') if '+' in expression else \\\n",
        "                    expression.split('-') if '-' in expression else \\\n",
        "                    expression.split('*') if '*' in expression else \\\n",
        "                    expression.split('/')\n",
        "\n",
        "            # Get the DataFrame column names involved in the operation\n",
        "            cols = [col for col in parts]\n",
        "\n",
        "            # Perform the corresponding arithmetic operation based on the operator in the expression\n",
        "            if cols[0] in train_df.columns and cols[1] in train_df.columns:\n",
        "                if '+' in expression:\n",
        "                    train_df[expression] = train_df[cols[0]] + train_df[cols[1]]\n",
        "                    test_df[expression] = test_df[cols[0]] + test_df[cols[1]]\n",
        "                elif '-' in expression:\n",
        "                    train_df[expression] = train_df[cols[0]] - train_df[cols[1]]\n",
        "                    test_df[expression] = test_df[cols[0]] - test_df[cols[1]]\n",
        "                elif '*' in expression:\n",
        "                    train_df[expression] = train_df[cols[0]] * train_df[cols[1]]\n",
        "                    test_df[expression] = test_df[cols[0]] * test_df[cols[1]]\n",
        "                elif '/' in expression:\n",
        "                    train_df[expression] = train_df[cols[0]] / (train_df[cols[1]]+1e-5)\n",
        "                    test_df[expression] = test_df[cols[0]] /( test_df[cols[1]]+1e-5)\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "train, test = apply_arithmetic_operations(train, test, new_cols)\n"
      ],
      "metadata": {
        "id": "EcDAsmSF_CnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_features=[f for f in train.columns if f not in [target]]\n",
        "final_features=[*set(final_features)]\n",
        "\n",
        "sc=StandardScaler()\n",
        "\n",
        "train_scaled=train.copy()\n",
        "test_scaled=test.copy()\n",
        "train_scaled[final_features]=sc.fit_transform(train[final_features])\n",
        "test_scaled[final_features]=sc.transform(test[final_features])"
      ],
      "metadata": {
        "id": "N0QrsltqTlkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_cop, test_cop=fill_missing_numerical(train_cop, test_cop,target, max_iterations=2)\n",
        "\n",
        "missing_cols=[f for f in test_cop.columns if train_cop[f].isna().sum()+test_cop[f].isna().sum()>0]\n",
        "\n",
        "for f in missing_cols:\n",
        "    train_cop[f]=train_cop[f].fillna(train_cop[f].median())\n",
        "    test_cop[f]=test_cop[f].fillna(test_cop[f].median())"
      ],
      "metadata": {
        "id": "IxnYN0_mTl_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train_cop.drop(columns=[target])\n",
        "y_train = train[target]\n",
        "\n",
        "X_test = test_cop.copy()\n",
        "\n",
        "print(X_train.shape, X_test.shape)"
      ],
      "metadata": {
        "id": "IKJqrZJXTqSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_most_important_features(X_train, y_train, n,model_input):\n",
        "    xgb_params = {\n",
        "            'n_estimators': 200,\n",
        "            'learning_rate': 0.05,\n",
        "            'max_depth': 4,\n",
        "            'subsample': 0.8,\n",
        "            'colsample_bytree': 0.1,\n",
        "            'n_jobs': -1,\n",
        "            'eval_metric': 'mlogloss',\n",
        "            'objective': 'multi:softprob',\n",
        "            'tree_method': 'hist',\n",
        "            'verbosity': 0,\n",
        "            'random_state': 42,\n",
        "        }\n",
        "    if device.lower() == 'gpu':\n",
        "            xgb_params['tree_method'] = 'gpu_hist'\n",
        "            xgb_params['predictor'] = 'gpu_predictor'\n",
        "    lgb_params = {\n",
        "            'n_estimators': 200,\n",
        "            'max_depth': 7,\n",
        "            'learning_rate': 0.05,\n",
        "            'subsample': 0.20,\n",
        "            'colsample_bytree': 0.56,\n",
        "            'reg_alpha': 0.25,\n",
        "            'reg_lambda': 5e-08,\n",
        "            'objective': 'multiclass',\n",
        "            'metric': 'multi_logloss',\n",
        "            'boosting_type': 'gbdt',\n",
        "            'random_state': 42,\n",
        "            'device': device.lower(),\n",
        "            'verbose':-1\n",
        "        }\n",
        "    cb_params = {\n",
        "            'iterations': 200,\n",
        "            'depth': 7,\n",
        "            'learning_rate': 0.1,\n",
        "            'l2_leaf_reg': 0.7,\n",
        "            'random_strength': 0.2,\n",
        "            'max_bin': 200,\n",
        "            'od_wait': 65,\n",
        "            'one_hot_max_size': 70,\n",
        "            'grow_policy': 'Depthwise',\n",
        "            'bootstrap_type': 'Bayesian',\n",
        "            'od_type': 'Iter',\n",
        "            'eval_metric': 'MultiClass',\n",
        "            'loss_function': 'MultiClass',\n",
        "            'task_type': device.upper(),\n",
        "            'random_state': 42,\n",
        "        }\n",
        "    if 'xgb' in model_input:\n",
        "        model = xgb.XGBClassifier(**xgb_params)\n",
        "    elif 'cat' in model_input:\n",
        "        model=CatBoostClassifier(**cb_params)\n",
        "    else:\n",
        "        model=lgb.LGBMClassifier(**lgb_params)\n",
        "\n",
        "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    f1_scores = []\n",
        "    feature_importances_list = []\n",
        "\n",
        "    for train_idx, val_idx in kfold.split(X_train):\n",
        "        X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "        if 'lgb' in model_input:\n",
        "            model.fit(X_train_fold, encode(y_train_fold,target_map))\n",
        "        else:\n",
        "            model.fit(X_train_fold,encode(y_train_fold,target_map),verbose=False)\n",
        "\n",
        "        y_pred = model.predict(X_val_fold)\n",
        "        f1_scores.append(accuracy_score(encode(y_val_fold,target_map), y_pred))\n",
        "        feature_importances = model.feature_importances_\n",
        "        feature_importances_list.append(feature_importances)\n",
        "\n",
        "    avg_f1 = np.mean(f1_scores)\n",
        "    avg_feature_importances = np.mean(feature_importances_list, axis=0)\n",
        "\n",
        "    feature_importance_list = [(X_train.columns[i], importance) for i, importance in enumerate(avg_feature_importances)]\n",
        "    sorted_features = sorted(feature_importance_list, key=lambda x: x[1], reverse=True)\n",
        "    top_n_features = [feature[0] for feature in sorted_features[:n]]\n",
        "\n",
        "    display_features=top_n_features[:12]\n",
        "\n",
        "    sns.set_palette([(0.8, 0.5, 0.3)])\n",
        "    plt.figure(figsize=(10, 12))\n",
        "    plt.barh(range(len(display_features)), [avg_feature_importances[X_train.columns.get_loc(feature)] for feature in display_features])\n",
        "    plt.yticks(range(len(display_features)), display_features, fontsize=12)\n",
        "    plt.xlabel('Average Feature Importance', fontsize=14)\n",
        "    plt.ylabel('Features', fontsize=10)\n",
        "    plt.title(f'Top {len(display_features)} of {n} Feature Importances with best Accuracy score {avg_f1}', fontsize=16)\n",
        "    plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature on top\n",
        "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "    plt.xticks(fontsize=8)\n",
        "    plt.yticks(fontsize=8)\n",
        "\n",
        "    # Add data labels on the bars\n",
        "    for index, value in enumerate([avg_feature_importances[X_train.columns.get_loc(feature)] for feature in display_features]):\n",
        "        plt.text(value + 0.005, index, f'{value:.3f}', fontsize=12, va='center')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return top_n_features"
      ],
      "metadata": {
        "id": "D1juTLB6Tt1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_imp_features_cat=get_most_important_features(X_train.reset_index(drop=True), y_train,150, 'cat')\n",
        "n_imp_features_xgb=get_most_important_features(X_train.reset_index(drop=True), y_train,150, 'xgb')\n",
        "n_imp_features_lgbm=get_most_important_features(X_train.reset_index(drop=True), y_train,150, 'lgbm')"
      ],
      "metadata": {
        "id": "4CmV2CMFTv2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_imp_features=[*set(n_imp_features_xgb+n_imp_features_lgbm+n_imp_features_cat)]\n",
        "print(f\"{len(n_imp_features)} features have been selected from three algorithms for the final model\")\n",
        "\n",
        "X_train=X_train[n_imp_features]\n",
        "X_test=X_test[n_imp_features]"
      ],
      "metadata": {
        "id": "G5yxyG0sTxyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = np.unique(y_train)  # Get unique class labels\n",
        "class_to_index = {cls: idx for idx, cls in enumerate(classes)}\n",
        "y_train_numeric = np.array([class_to_index[cls] for cls in y_train])\n",
        "\n",
        "class_counts = np.bincount(y_train_numeric)\n",
        "\n",
        "total_samples = len(y_train_numeric)\n",
        "\n",
        "class_weights = total_samples / (len(classes) * class_counts)\n",
        "\n",
        "class_weights_dict = {target_map[cls]: weight for cls, weight in zip(classes, class_weights)}\n",
        "\n",
        "print(\"Class counts:\", class_counts)\n",
        "print(\"Total samples:\", total_samples)\n",
        "print(\"Class weights:\", class_weights)\n",
        "print(\"Class weights dictionary:\", class_weights_dict)"
      ],
      "metadata": {
        "id": "c2pP0PzlT0on"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.layers import LeakyReLU, PReLU, ELU\n",
        "from keras.layers import Dropout\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "gpus = tensorflow.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    print(\"GPU is not available\")\n",
        "\n",
        "\n",
        "def optimizer():\n",
        "    sgd=tensorflow.keras.optimizers.SGD(learning_rate=0.005, momentum=0.5, nesterov=True)\n",
        "    rms = tensorflow.keras.optimizers.RMSprop()\n",
        "    nadam=tensorflow.keras.optimizers.Nadam(\n",
        "        learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Nadam\"\n",
        "    )\n",
        "    adam=tensorflow.keras.optimizers.Adam()\n",
        "    adamW = keras.optimizers.AdamW(learning_rate=0.002,weight_decay=0.001, beta_1=0.9, beta_2=0.999)\n",
        "\n",
        "    return sgd,rms,nadam, adamW,adam\n",
        "\n",
        "\n",
        "lrelu = lambda x: tensorflow.keras.activations.relu(x, alpha=0.1)\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def root_mean_squared_log_error(y_true, y_pred):\n",
        "    '''\n",
        "    Compute RMSLE between actuals & predictions\n",
        "    '''\n",
        "    return K.sqrt(K.mean(K.square(K.log(abs(y_pred+1)) - K.log(abs(y_true+1)))))\n",
        "\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "    '''\n",
        "    Compute RMSE between actuals & predictions\n",
        "    '''\n",
        "    return K.sqrt(K.mean(K.square(y_pred - y_true)))"
      ],
      "metadata": {
        "id": "5UOhITP9y7L6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_ann1(num_classes, input_dim):\n",
        "    '''\n",
        "    Initialize the artificial neural network (ANN) for multiclass classification\n",
        "    '''\n",
        "\n",
        "    sgd, rms, nadam, adamW, adam = optimizer()\n",
        "\n",
        "    ann = Sequential()\n",
        "    ann.add(Dense(288, input_dim=input_dim, kernel_initializer='he_uniform', activation='relu'))\n",
        "    ann.add(Dropout(0.4))\n",
        "    ann.add(Dense(80, kernel_initializer='he_uniform', activation='relu'))\n",
        "    ann.add(Dropout(0.1))\n",
        "    ann.add(Dense(32, kernel_initializer='he_uniform', activation='relu'))\n",
        "    ann.add(Dropout(0.0))\n",
        "\n",
        "    # Change the output layer to match the number of classes\n",
        "    ann.add(Dense(num_classes, kernel_initializer='he_uniform', activation='softmax'))\n",
        "\n",
        "    # Compile the model with categorical_crossentropy loss and accuracy metric\n",
        "    ann.compile(loss='categorical_crossentropy', optimizer=nadam, metrics=['accuracy'])\n",
        "\n",
        "    return ann\n",
        "\n",
        "def init_ann2(num_classes, input_dim):\n",
        "    sgd,rms,nadam, adamW, adam=optimizer()\n",
        "    ann2 = Sequential()\n",
        "    ann2.add(Dense(128, input_dim=X_test.shape[1], kernel_initializer='he_uniform', activation='relu'))\n",
        "    ann2.add(Dropout(0.3))\n",
        "    ann2.add(Dense(52,  kernel_initializer='he_uniform', activation='relu'))\n",
        "    ann2.add(Dropout(0.1))\n",
        "    ann2.add(Dense(12,  kernel_initializer='he_uniform', activation='relu'))\n",
        "    ann2.add(Dropout(0.2))\n",
        "#     ann2.add(Dense(16,  kernel_initializer='he_uniform', activation='relu'))\n",
        "#     ann2.add(Dropout(0.1))\n",
        "\n",
        "    ann2.add(Dense(num_classes, kernel_initializer='he_uniform', activation='softmax'))\n",
        "    ann2.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "\n",
        "    return ann2\n",
        "\n",
        "def init_ann3(num_classes, input_dim):\n",
        "    sgd,rms,nadam, adamW, adam=optimizer()\n",
        "    ann3 = Sequential()\n",
        "    ann3.add(Dense(384, input_dim=X_test.shape[1], kernel_initializer='he_uniform', activation='relu'))\n",
        "    ann3.add(Dropout(0.3))\n",
        "    ann3.add(Dense(84,  kernel_initializer='he_uniform', activation='relu'))\n",
        "    ann3.add(Dropout(0.0))\n",
        "#     ann3.add(Dense(112,  kernel_initializer='he_uniform', activation='relu'))\n",
        "#     ann3.add(Dropout(0.1))\n",
        "#     ann3.add(Dense(4,  kernel_initializer='he_uniform', activation='relu'))\n",
        "#     ann3.add(Dropout(0.1))\n",
        "\n",
        "    ann3.add(Dense(num_classes, kernel_initializer='he_uniform', activation='softmax'))\n",
        "    ann3.compile(loss='categorical_crossentropy', optimizer=adamW, metrics=['accuracy'])\n",
        "\n",
        "    return ann3\n",
        "\n",
        "def init_ann4(num_classes, input_dim):\n",
        "    sgd,rms,nadam, adamW, adam=optimizer()\n",
        "    ann4 = Sequential()\n",
        "    ann4.add(Dense(128, input_dim=X_test.shape[1], kernel_initializer='he_uniform', activation='relu'))\n",
        "    ann4.add(Dropout(0.5))\n",
        "    ann4.add(Dense(16,  kernel_initializer='he_uniform', activation='relu'))\n",
        "    ann4.add(Dropout(0.3))\n",
        "    # ann4.add(Dense(32,  kernel_initializer='he_uniform', activation='relu'))\n",
        "    # ann4.add(Dropout(0.3))\n",
        "    ann4.add(Dense(4,  kernel_initializer='he_uniform', activation='relu'))\n",
        "    ann4.add(Dropout(0.1))\n",
        "\n",
        "    ann4.add(Dense(num_classes, kernel_initializer='he_uniform', activation='softmax'))\n",
        "    ann4.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "    return ann4\n",
        "\n",
        "def init_ann5(num_classes, input_dim):\n",
        "    sgd,rms,nadam, adamW, adam=optimizer()\n",
        "    ann5 = Sequential()\n",
        "    ann5.add(Dense(96, input_dim=X_test.shape[1], kernel_initializer='he_uniform', activation='relu'))\n",
        "    ann5.add(Dropout(0.4))\n",
        "    ann5.add(Dense(180,  kernel_initializer='he_uniform', activation='relu'))\n",
        "#     ann5.add(Dropout(0.1))\n",
        "\n",
        "    ann5.add(Dense(num_classes, kernel_initializer='he_uniform', activation='softmax'))\n",
        "    ann5.compile(loss='categorical_crossentropy', optimizer=rms, metrics=['accuracy'])\n",
        "    return ann5"
      ],
      "metadata": {
        "id": "XUxfTwCXzCnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Splitter:\n",
        "    def __init__(self, test_size=0.2, kfold=True, n_splits=5):\n",
        "        self.test_size = test_size\n",
        "        self.kfold = kfold\n",
        "        self.n_splits = n_splits\n",
        "\n",
        "    def split_data(self, X, y, random_state_list):\n",
        "        if self.kfold:\n",
        "            for random_state in random_state_list:\n",
        "                kf = StratifiedKFold(n_splits=self.n_splits, random_state=random_state, shuffle=True)\n",
        "                for train_index, val_index in kf.split(X, y):\n",
        "                    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
        "                    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
        "                    yield X_train, X_val, y_train, y_val\n",
        "\n",
        "class Classifier:\n",
        "    def __init__(self, n_estimators=100, device=\"cpu\", random_state=0):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.device = device\n",
        "        self.random_state = random_state\n",
        "        self.models = self._define_model()\n",
        "        self.len_models = len(self.models)\n",
        "\n",
        "    def _define_model(self):\n",
        "\n",
        "        xgb_params = {\n",
        "            'n_estimators': self.n_estimators,\n",
        "            'learning_rate': 0.05,\n",
        "            'max_depth': 4,\n",
        "            'subsample': 0.8,\n",
        "            'colsample_bytree': 0.1,\n",
        "            'n_jobs': -1,\n",
        "            'eval_metric': 'merror',\n",
        "            'objective': 'multi_logloss',\n",
        "            'tree_method': 'hist',\n",
        "            'verbosity': 0,\n",
        "            'random_state': self.random_state,\n",
        "            'class_weight':class_weights_dict,\n",
        "        }\n",
        "        if self.device == 'gpu':\n",
        "            xgb_params['tree_method'] = 'gpu_hist'\n",
        "            xgb_params['predictor'] = 'gpu_predictor'\n",
        "\n",
        "        xgb_params2 = {\n",
        "            'n_estimators': self.n_estimators,\n",
        "            'gamma': 0.2785290882258914,\n",
        "            'max_depth': 10,\n",
        "            'subsample': 0.32478148672785395,\n",
        "             'min_child_weight': 9,\n",
        "            'colsample_bytree': 0.4867041443764002,\n",
        "            'learning_rate': 0.0521164563894605,\n",
        "            'reg_lambda': 0.0006976301054985199,\n",
        "            'reg_alpha': 0.37133730627394207,\n",
        "            'n_jobs': -1,\n",
        "            'eval_metric': 'merror',\n",
        "            'objective': 'multi_logloss',\n",
        "            'tree_method': 'hist',\n",
        "            'verbosity': 0,\n",
        "            'random_state': self.random_state,\n",
        "            'class_weight':class_weights_dict,\n",
        "        }\n",
        "        if self.device == 'gpu':\n",
        "            xgb_params2['tree_method'] = 'gpu_hist'\n",
        "            xgb_params2['predictor'] = 'gpu_predictor'\n",
        "\n",
        "\n",
        "        xgb_params3 = {\n",
        "            'n_estimators': self.n_estimators,\n",
        "            'gamma': 0.2785290882258914,\n",
        "            'max_depth': 10,\n",
        "            'subsample': 0.32478148672785395,\n",
        "             'min_child_weight': 9,\n",
        "            'colsample_bytree': 0.4867041443764002,\n",
        "            'learning_rate': 0.0521164563894605,\n",
        "            'reg_lambda': 0.0006976301054985199,\n",
        "            'reg_alpha': 0.37133730627394207,\n",
        "            'n_jobs': -1,\n",
        "            'eval_metric': 'merror',\n",
        "            'objective': 'multi_logloss',\n",
        "            'tree_method': 'hist',\n",
        "            'verbosity': 0,\n",
        "            'device': 'cuda',\n",
        "            'booster': 'gbtree',\n",
        "            'random_state': self.random_state,\n",
        "        }\n",
        "\n",
        "        xgb_params4=xgb_params.copy()\n",
        "        xgb_params4['subsample']= 0.7893839050813315\n",
        "        xgb_params4['max_depth']=5\n",
        "        xgb_params4['learning_rate']=0.161\n",
        "        xgb_params4['colsample_bytree']=0.2425879035225036\n",
        "\n",
        "        xgb_params5=xgb_params.copy()\n",
        "#         xgb_params5['subsample']= 0.8380895263029425\n",
        "#         xgb_params5['max_depth']=4\n",
        "#         xgb_params5['learning_rate']=0.097\n",
        "#         xgb_params5['colsample_bytree']=0.24119317177396393\n",
        "        xgb_params5['objective']=\"multi:softprob\"\n",
        "\n",
        "        lgb_params = {\n",
        "            'n_estimators': 1024,\n",
        "            'max_depth': 11,\n",
        "            'min_samples_leaf': 26,\n",
        "            'subsample': 0.9118977524246277,\n",
        "            'learning_rate': 0.028851311126998066,\n",
        "            'lambda_l1': 4.1605396291792305,\n",
        "            'lambda_l2': 2.6356292583061972e-05,\n",
        "            'colsample_bytree': 0.2060085083494746,\n",
        "#             'max_bin':245,\n",
        "            'objective': 'multiclass',\n",
        "            'metric': 'multi_logloss',\n",
        "            'boosting_type': 'gbdt',\n",
        "            'device': self.device,\n",
        "            'random_state': self.random_state,\n",
        "#             'class_weight':class_weights_dict,\n",
        "            'verbose':-1\n",
        "        }\n",
        "\n",
        "        lgb_params2 = {\n",
        "            'n_estimators': 1000,\n",
        "            'max_depth': 6,\n",
        "            'subsample': 0.7429283449560508,\n",
        "            'learning_rate': 0.049029736097283075,\n",
        "            'lambda_l1': 8.921656056016416e-05,\n",
        "            'lambda_l2': 0.0017743761809582512,\n",
        "            'colsample_bytree': 0.39212030104410694,\n",
        "            'objective': 'multiclass',\n",
        "            'metric': 'multi_error',\n",
        "            'boosting_type': 'gbdt',\n",
        "            'device': self.device,\n",
        "            'random_state': self.random_state,\n",
        "            'verbose':-1\n",
        "        }\n",
        "\n",
        "        lgb_params3 = {\n",
        "            'n_estimators': 1000,\n",
        "            'max_depth': 9,\n",
        "            'subsample': 0.5396835462409598,\n",
        "            'learning_rate': 0.049047285930431804,\n",
        "            'lambda_l1': 1.7490663112850725e-08,\n",
        "            'lambda_l2': 3.837216667252457,\n",
        "            'colsample_bytree': 0.3193726625999373,\n",
        "            'objective': 'multiclass',\n",
        "            'metric': 'multi_error',\n",
        "            'boosting_type': 'gbdt',\n",
        "            'device': self.device,\n",
        "            'random_state': self.random_state,\n",
        "            'verbose':-1\n",
        "        }\n",
        "        lgb_params4=lgb_params2.copy()\n",
        "        lgb_params4['subsample']=0.9\n",
        "        lgb_params4['reg_lambda']=0.8762427869944874\n",
        "        lgb_params4['reg_alpha']=0.31889688644219183\n",
        "        lgb_params4['max_depth']=9\n",
        "        lgb_params4['learning_rate']=0.107\n",
        "        lgb_params4['colsample_bytree']=0.1\n",
        "\n",
        "        lgb_params5=lgb_params2.copy()\n",
        "        lgb_params5['subsample']=0.9\n",
        "        lgb_params5['reg_lambda']=0.5119821036432592\n",
        "        lgb_params5['reg_alpha']=0.8978283976480275\n",
        "        lgb_params5['max_depth']=11\n",
        "        lgb_params5['learning_rate']=0.081\n",
        "        lgb_params5['colsample_bytree']=0.1\n",
        "        cb_params = {\n",
        "            'iterations': self.n_estimators,\n",
        "            'depth': 6,\n",
        "            'learning_rate': 0.05,\n",
        "            'l2_leaf_reg': 0.7,\n",
        "            'random_strength': 0.2,\n",
        "            'max_bin': 200,\n",
        "            'od_wait': 65,\n",
        "            'one_hot_max_size': 70,\n",
        "            'grow_policy': 'Depthwise',\n",
        "            'bootstrap_type': 'Bayesian',\n",
        "            'od_type': 'Iter',\n",
        "            'eval_metric': 'Accuracy',\n",
        "            'loss_function': 'MultiClass',\n",
        "            'task_type': self.device.upper(),\n",
        "            'random_state': self.random_state,\n",
        "            'verbose':-1\n",
        "\n",
        "        }\n",
        "        cb_sym_params = cb_params.copy()\n",
        "        cb_sym_params['grow_policy'] = 'SymmetricTree'\n",
        "        cb_loss_params = cb_params.copy()\n",
        "        cb_loss_params['grow_policy'] = 'Lossguide'\n",
        "\n",
        "        cb_params2=  cb_params.copy()\n",
        "        cb_params2['learning_rate']=0.019000000000000003\n",
        "        cb_params2['depth']=9\n",
        "        cb_params2['random_strength']=0.3\n",
        "        cb_params2['one_hot_max_size']=10\n",
        "        cb_params2['max_bin']=100\n",
        "        cb_params2['l2_leaf_reg']=0.4189770851283918\n",
        "#         cb_params2['bootstrap_type']='Bernoulli'\n",
        "\n",
        "        cb_params3={\n",
        "            'iterations': self.n_estimators,\n",
        "            'random_strength': 0.1,\n",
        "            'one_hot_max_size': 70, 'max_bin': 100,\n",
        "            'learning_rate': 0.008,\n",
        "            'l2_leaf_reg': 0.3,\n",
        "            'grow_policy': 'Depthwise',\n",
        "            'depth': 9,\n",
        "            'max_bin': 200,\n",
        "            'od_wait': 65,\n",
        "            'bootstrap_type': 'Bayesian',\n",
        "            'od_type': 'Iter',\n",
        "            'eval_metric': 'Accuracy',\n",
        "            'loss_function': 'MultiClass',\n",
        "            'task_type': self.device.upper(),\n",
        "            'random_state': self.random_state,\n",
        "        }\n",
        "        cb_params4=  cb_params.copy()\n",
        "        cb_params4['learning_rate']=0.14300000000000002\n",
        "        cb_params4['depth']=16\n",
        "        cb_params4['random_strength']=0.5959959004533222\n",
        "        cb_params4['one_hot_max_size']=100\n",
        "        cb_params4['max_bin']=150\n",
        "        cb_params4['l2_leaf_reg']=0.3838383515991557\n",
        "        cb_params4['grow_policy']='Lossguide'\n",
        "\n",
        "        dt_params= {'criterion': 'gini', 'max_depth': 9, 'min_samples_split': 17, 'min_samples_leaf': 18, 'max_features': 0.8433062331953287}\n",
        "        etr_params= {'criterion': 'gini', 'max_depth': 16, 'min_samples_split': 11, 'min_samples_leaf': 1, 'max_features': 0.6678697051089325, 'bootstrap': True}\n",
        "        hist_params= {'learning_rate': 0.05835392348975443, 'n_iter_no_change': 795, 'max_depth': 4, 'min_samples_leaf': 17, 'max_leaf_nodes': 98, 'l2_regularization': 1.923251027172902e-07}\n",
        "        rf_params={'max_depth': 16, 'min_samples_split': 18, 'min_samples_leaf': 2, 'max_features': 0.4156475826924276}\n",
        "        gbt_params={'learning_rate': 0.13640097404424165, 'max_depth': 7, 'min_samples_split': 17, 'min_samples_leaf': 15, 'subsample': 0.8858460576511749, 'max_features': 0.6114927460685724}\n",
        "        knn_params={'n_neighbors': 16, 'weights': 'uniform', 'p': 2, 'leaf_size': 13, 'algorithm': 'ball_tree'}\n",
        "        adb_params={'n_estimators': 957, 'learning_rate': 0.6625214812098597}\n",
        "\n",
        "        models = {\n",
        "###########             'svc': SVC(gamma=\"auto\", probability=True, random_state=self.random_state),\n",
        "            'xgb':  xgb.XGBClassifier(**xgb_params),\n",
        "#             'xgb2': xgb.XGBClassifier(**xgb_params2),\n",
        "#             'xgb3': xgb.XGBClassifier(**xgb_params3),\n",
        "            'xgb4': xgb.XGBClassifier(**xgb_params4),\n",
        "#             'xgb5': xgb.XGBClassifier(**xgb_params5),\n",
        "           'lgb':  lgb.LGBMClassifier(**lgb_params),\n",
        "            'lgb2': lgb.LGBMClassifier(**lgb_params2),\n",
        "#             'lgb3': lgb.LGBMClassifier(**lgb_params3),\n",
        "#             'lgb4': lgb.LGBMClassifier(**lgb_params4),\n",
        "           'lgb5': lgb.LGBMClassifier(**lgb_params5),\n",
        "#             'cat':  CatBoostClassifier(**cb_params),\n",
        "#             'cat2': CatBoostClassifier(**cb_params2),\n",
        "#             'cat3': CatBoostClassifier(**cb_params3),\n",
        "#             'cat4': CatBoostClassifier(**cb_params4),\n",
        "            \"cat_sym\": CatBoostClassifier(**cb_sym_params),\n",
        "#             \"cat_loss\": CatBoostClassifier(**cb_loss_params),\n",
        "#             'brf': BalancedRandomForestClassifier(n_estimators=4000,max_depth=12 ,n_jobs=-1, random_state=self.random_state),\n",
        "            'rf': RandomForestClassifier(n_estimators=250,**rf_params, random_state=self.random_state),\n",
        "            'hist_gbm' : HistGradientBoostingClassifier(max_iter=self.n_estimators,**hist_params,class_weight=class_weights_dict, random_state=self.random_state),\n",
        "            'gbdt': GradientBoostingClassifier(**gbt_params,n_estimators=100,random_state=self.random_state),\n",
        "            'ada': AdaBoostClassifier(**adb_params,random_state=self.random_state),\n",
        "            'etr': ExtraTreesClassifier(**etr_params,random_state=self.random_state),\n",
        "            'dt' : DecisionTreeClassifier(**dt_params,random_state=self.random_state),\n",
        "#             'knn': KNeighborsClassifier(**knn_params),\n",
        "            'log_reg': LogisticRegression(multi_class = 'ovr', max_iter = 1000),\n",
        "            'ridge': CalibratedClassifierCV(RidgeClassifierCV(alphas=100.02351202464449), method='sigmoid'),\n",
        "            'elasticNet':LogisticRegressionCV(Cs= [0.04426763148249415], l1_ratios= [0.9771433916131904]),\n",
        "#             'ann1':init_ann1(3, X_test.shape[1]),\n",
        "            'ann2':init_ann2(3, X_test.shape[1]),\n",
        "#             'ann3':init_ann3(3, X_test.shape[1]),\n",
        "            'ann4':init_ann4(3, X_test.shape[1]),\n",
        "#             'ann5':init_ann5(3, X_test.shape[1]),\n",
        "\n",
        "\n",
        "        }\n",
        "        return models"
      ],
      "metadata": {
        "id": "wIsuxPMXzD4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OptunaWeights:\n",
        "    def __init__(self, random_state, n_trials=3000):\n",
        "        self.study = None\n",
        "        self.weights = None\n",
        "        self.random_state = random_state\n",
        "        self.n_trials = n_trials\n",
        "\n",
        "    def _objective(self, trial, y_true, y_preds):\n",
        "        # Define the weights for the predictions from each model\n",
        "        weights = [trial.suggest_float(f\"weight{n}\", 0, 1) for n in range(len(y_preds))]\n",
        "\n",
        "        # Calculate the weighted prediction\n",
        "        weighted_pred = np.average(np.array(y_preds), axis=0, weights=weights)\n",
        "        weighted_pred = weighted_pred/weighted_pred.sum(axis=1, keepdims=True)\n",
        "\n",
        "        weighted_pred_labels = np.argmax(weighted_pred, axis=1)\n",
        "        accuracy = accuracy_score(y_true, weighted_pred_labels)\n",
        "\n",
        "        log_loss_score=log_loss(y_true, weighted_pred)\n",
        "        return accuracy\n",
        "\n",
        "    def fit(self, y_true, y_preds):\n",
        "        optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
        "        sampler = optuna.samplers.CmaEsSampler(seed=self.random_state)\n",
        "        pruner = optuna.pruners.HyperbandPruner()\n",
        "        self.study = optuna.create_study(sampler=sampler, pruner=pruner, study_name=\"OptunaWeights\", direction='maximize')\n",
        "        objective_partial = partial(self._objective, y_true=y_true, y_preds=y_preds)\n",
        "        self.study.optimize(objective_partial, n_trials=self.n_trials)\n",
        "        self.weights = [self.study.best_params[f\"weight{n}\"] for n in range(len(y_preds))]\n",
        "\n",
        "    def predict(self, y_preds):\n",
        "        assert self.weights is not None, 'OptunaWeights error, must be fitted before predict'\n",
        "        weighted_pred = np.average(np.array(y_preds), axis=0, weights=self.weights)\n",
        "        return weighted_pred\n",
        "\n",
        "    def fit_predict(self, y_true, y_preds):\n",
        "        self.fit(y_true, y_preds)\n",
        "        return self.predict(y_preds)\n",
        "\n",
        "    def weights(self):\n",
        "        return self.weights"
      ],
      "metadata": {
        "id": "EHPQrOIVzKEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "\n",
        "kfold = True\n",
        "n_splits = 1 if not kfold else 6\n",
        "random_state = 42\n",
        "random_state_list = [42]\n",
        "n_estimators = 9999\n",
        "early_stopping_rounds = 600\n",
        "verbose = False\n",
        "splitter = Splitter(kfold=kfold, n_splits=n_splits)\n",
        "\n",
        "# Initialize an array for storing test predictions\n",
        "test_predss = np.zeros((X_test.shape[0], 3))\n",
        "ensemble_score = []\n",
        "ensemble_acc_score = []\n",
        "weights = []\n",
        "trained_models = {'xgb':[]}\n",
        "\n",
        "for i, (X_train_, X_val, y_train_, y_val) in enumerate(splitter.split_data(X_train, y_train, random_state_list=random_state_list)):\n",
        "    n = i % n_splits\n",
        "    m = i // n_splits\n",
        "\n",
        "    # Get a set of Regressor models\n",
        "    classifier = Classifier(n_estimators, device, random_state)\n",
        "    models = classifier.models\n",
        "\n",
        "    # Initialize lists to store oof and test predictions for each base model\n",
        "    oof_preds = []\n",
        "    test_preds = []\n",
        "    start_time_fold = time.time()\n",
        "\n",
        "    # Loop over each base model and fit it to the training data, evaluate on validation data, and store predictions\n",
        "    for name, model in models.items():\n",
        "        start_time = time.time()\n",
        "        if ('xgb' in name) or ('lgb' in name) or ('cat' in name)  :\n",
        "            if 'lgb' in name:\n",
        "                model.fit(X_train_, encode(y_train_,target_map), eval_set=[(X_val, encode(y_val,target_map))])\n",
        "            else:\n",
        "                model.fit(X_train_, encode(y_train_,target_map), eval_set=[(X_val, encode(y_val,target_map))], early_stopping_rounds=early_stopping_rounds, verbose=verbose)\n",
        "        elif 'ann' in name:\n",
        "            model.fit(X_train_, to_categorical(np.array(encode(y_train_,target_map)),3),\\\n",
        "                      validation_data=(X_val, to_categorical(np.array(encode(y_val,target_map)))),batch_size=10, epochs=50,verbose=verbose)#validation_data=(X_val, y_val)\n",
        "        else:\n",
        "            model.fit(X_train_, encode(y_train_,target_map))\n",
        "\n",
        "        if name in trained_models.keys():\n",
        "            trained_models[f'{name}'].append(deepcopy(model))\n",
        "\n",
        "        if 'ann' in name:\n",
        "            y_val_pred = np.array(model.predict(X_val))#[:,0]\n",
        "            test_pred = np.array(model.predict(X_test))\n",
        "        else:\n",
        "            test_pred = model.predict_proba(X_test)\n",
        "            y_val_pred = model.predict_proba(X_val)\n",
        "\n",
        "        end_time = time.time()\n",
        "        time_taken = end_time - start_time\n",
        "\n",
        "        y_val_pred_labels = np.argmax(y_val_pred, axis=1)\n",
        "        accuracy = accuracy_score(encode(y_val,target_map), y_val_pred_labels)\n",
        "\n",
        "        score = log_loss(encode(y_val,target_map), y_val_pred)\n",
        "        print(f'{name} [FOLD-{n} SEED-{random_state_list[m]}] Accuracy Score: {accuracy:.5f}, time taken :{time_taken:.3f} secs')\n",
        "\n",
        "        oof_preds.append(y_val_pred)\n",
        "        test_preds.append(test_pred)\n",
        "\n",
        "\n",
        "    # Use Optuna to find the best ensemble weights\n",
        "    optweights = OptunaWeights(random_state=random_state)\n",
        "    y_val_pred = optweights.fit_predict(encode(y_val,target_map), oof_preds)\n",
        "\n",
        "    score = log_loss(encode(y_val,target_map), y_val_pred)\n",
        "    y_val_pred_labels = np.argmax(y_val_pred, axis=1)\n",
        "    accuracy = accuracy_score(encode(y_val,target_map), y_val_pred_labels)\n",
        "\n",
        "    end_time_fold = time.time()\n",
        "    time_taken = end_time_fold - start_time_fold\n",
        "\n",
        "    print(f'Ensemble [FOLD-{n} SEED-{random_state_list[m]}] -------------------> Accuracy Score: {accuracy:.5f}, Logloss: {score:.5f}, fold time taken :{time_taken:.5f} secs')\n",
        "\n",
        "    ensemble_score.append(score)\n",
        "    ensemble_acc_score.append(accuracy)\n",
        "    weights.append(optweights.weights)\n",
        "\n",
        "    # Predict to X_test by the best ensemble weights\n",
        "    _test_preds = optweights.predict(test_preds)\n",
        "    test_predss += _test_preds / (n_splits * len(random_state_list))\n",
        "\n",
        "    gc.collect()\n"
      ],
      "metadata": {
        "id": "uvGShpAnN-Yk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the mean LogLoss score of the ensemble\n",
        "mean_score = np.mean(ensemble_acc_score)\n",
        "std_score = np.std(ensemble_acc_score)\n",
        "print(f'Ensemble Accuracy score {mean_score:.5f} ± {std_score:.5f}')\n",
        "\n",
        "# Print the mean and standard deviation of the ensemble weights for each model\n",
        "print('--- Model Weights ---')\n",
        "mean_weights = np.mean(weights, axis=0)\n",
        "std_weights = np.std(weights, axis=0)\n",
        "for name, mean_weight, std_weight in zip(models.keys(), mean_weights, std_weights):\n",
        "    print(f'{name}: {mean_weight:.5f} ± {std_weight:.5f}')"
      ],
      "metadata": {
        "id": "0MX6AaMHOE3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_importance(models, feature_cols, title, head=15):\n",
        "    importances = []\n",
        "    feature_importance = pd.DataFrame()\n",
        "    for i, model in enumerate(models):\n",
        "        _df = pd.DataFrame()\n",
        "        _df[\"importance\"] = model.feature_importances_\n",
        "        _df[\"feature\"] = pd.Series(feature_cols)\n",
        "        _df[\"fold\"] = i\n",
        "        _df = _df.sort_values('importance', ascending=False)\n",
        "        _df = _df.head(head)\n",
        "        feature_importance = pd.concat([feature_importance, _df], axis=0, ignore_index=True)\n",
        "\n",
        "    feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
        "    # display(feature_importance.groupby([\"feature\"]).mean().reset_index().drop('fold', axis=1))\n",
        "    plt.figure(figsize=(18, 10))\n",
        "    sns.barplot(x='importance', y='feature', data=feature_importance, color= (0.8, 0.5, 0.3), errorbar='sd')\n",
        "    plt.xlabel('Importance', fontsize=14)\n",
        "    plt.ylabel('Feature', fontsize=14)\n",
        "    plt.title(f'{title} Feature Importance', fontsize=18)\n",
        "    plt.grid(True, axis='x')\n",
        "    plt.show()\n",
        "\n",
        "for name, models in trained_models.items():\n",
        "    visualize_importance(models, list(X_train.columns), name)"
      ],
      "metadata": {
        "id": "hc1jlQp7OH30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission[target] =   decode(np.argmax(test_predss, axis=1),target_map)\n",
        "submission.to_csv('submission_pure.csv',index=False)\n",
        "submission.head()"
      ],
      "metadata": {
        "id": "_0SZlMO0OOXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2"
      ],
      "metadata": {
        "id": "BVSIQy8zORDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "id": "-KAVSbrPOSgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "%matplotlib inline\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "import optuna\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import StackingClassifier, VotingClassifier, RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, cross_validate, cross_val_predict, RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_selection import SequentialFeatureSelector"
      ],
      "metadata": {
        "id": "gSAIwiiSHkpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df=pd.read_csv(\"/kaggle/input/playground-series-s4e6/train.csv\")\n",
        "test_df=pd.read_csv('/kaggle/input/playground-series-s4e6/test.csv')"
      ],
      "metadata": {
        "id": "HuUHwf5wHqdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "id": "bsahRP-KHt1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.duplicated().sum()"
      ],
      "metadata": {
        "id": "hw4c115hHwEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.info()"
      ],
      "metadata": {
        "id": "7ur_vnfdC3cG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.describe()"
      ],
      "metadata": {
        "id": "NiTYLny6C4cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_features = list(train_df.columns)\n",
        "init_features"
      ],
      "metadata": {
        "id": "kJmukofOC8BT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in train_df.columns:\n",
        "    print(f'{col} has {train_df[col].nunique()} unique values')"
      ],
      "metadata": {
        "id": "I1YsBRf-C9QW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 10))\n",
        "ax = sns.countplot(x='Target', data=train_df)\n",
        "\n",
        "# Add labels to each bar in the plot\n",
        "for p in ax.patches:\n",
        "    ax.text(p.get_x() + p.get_width() / 2, p.get_height() + 3, f'{int(p.get_height())}', ha=\"center\")\n",
        "\n",
        "plt.xlabel('Target Variable')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Target Variable Distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OLu_QzStC_4F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}